TITLE:: RTNeural
summary:: A Real-Time Neural Inferencing UGen
categories:: UGens>NeuralProcessing


DESCRIPTION::
RTNeural is a SuperCollider UGen which uses the RTNeural inference engine to load and run tensorflow neural network models. See the RTNeural github page (https://github.com/jatinchowdhury18/RTNeural) for neural network layers supported.  

Be sure the check out mlp_control_tutorial.scd in the mlp_control_tutorial folder for a control rate multi-layer perceptron tutorial.

CLASSMETHODS::

METHOD:: ar
RTNeural models can operate at audio rate or control rate. At audio rate the entire input array should be audio rate. At control rate, the entire input array should be control rate.

ARGUMENT:: input_array
an array of audio/control inputs - size of the array needs to match the size of the input layer of the provided neural network model

ARGUMENT:: num_outputs
the number of outputs for the UGen. this number must match the number of outputs of the provided neural network model

ARGUMENT:: id
each RTNeural UGen in a Synth needs a unique id so that sclang can send messages to the correct instance of the UGen

ARGUMENT:: bypass
bypass the effect - audio goes straight out with no CPU use by the neural network

ARGUMENT:: sample_rate
the default setting is -1, which the signal is processed at the current sample rate. however, some audio effects are trained on audio at a specific sample rate. if the provided model is trained a specific sample rate, indicate that sample rate here. when the provided sample rate is different from that of the audio driver, the input array will be resampled to the sample rate of the neural network before inference and sampled back to the driver's sample rate after.

ARGUMENT:: trig_mode
normally this will be set to 0, which will run inference at audio rate or control rate. however, when set to 1, the ugen will only run inference when the "trigger" argument is above 0. This allows inference to happen on any sample, but not all samples.

ARGUMENT:: trigger
An audio rate or control rate trigger which will activate inference at sample accurate timing. Only works when trig_mode is set to 1.

ARGUMENT:: reset
rtneural RNN models are stateful. in order to create a stateless RNN, send a trigger to the reset argument before each inference stream. resets will only work if RTNeural is in trigger mode and there is no sample_rate correction.

METHOD:: kr
RTNeural models can operate at audio rate or control rate. At audio rate the input array should be audio rate. At control rate, the input array should be control rate.

METHOD:: loadModel
loads pretrained RTNeural models and weights into the RTNeural C++ library from an RTNeural format json file. keras models can be saved in the correct format using the 'save_model' function from the provided 'model_utils.py' file. pytorch models need to be converted to keras models before using the save_models function. see the python examples and examples showing model conversion from pytorch to keras/tensorflow.

ARGUMENT:: synth
the synth where the UGen exists

ARGUMENT:: id
the id for the UGen where you are loading the model

ARGUMENT:: path
path to the json file training

ARGUMENT:: verbose
default is true. writes information about the network to the screen while loading.

EXAMPLES::

----------------Audio Rate Recurrent Neural Networks---------------

Proteus and other Automated-GuitarAmpModelling GRU and LSTM trainings

code::
//RTNeural can load LSTM and GRU trainings that were made using Alec Wright's Automated-GuitarAmpModelling python (https://github.com/Alec-Wright/Automated-GuitarAmpModelling)
//the python code to transform these models from pytorch to keras/RTNeural format is provided
(
    s.waitForBoot({
        p = Platform.resourceDir +/+ "sounds/a11wlk01.wav";
        b = Buffer.read(s, p);
        s.sync;
        SynthDef('rt_neural_dist', {
            var in = PlayBuf.ar(1, \buf_num.kr(0), BufRateScale.kr(\buf_dur.kr), loop:1);
    
            //models trained with the Automated-GuitarAmpModelling library usually use a skip connection in the training, so we need to add the signal back into itself to get the output of the original circuit
            //the two trainings below al
            var synth = RTNeural.ar([in], 1, 'this_one', \bypass.kr(0), \sr.kr(44100))+in*0.5;
            Out.ar(0, synth.dup);
        }).load(s);
    })
)

//make the synth
~synth = Synth('rt_neural_dist', [\buf_num, b]);

//load an LSTM model from the Proteus guitar pedal
//these trainings use an LSTM with a hidden layer of 20 and were trained at 44100
(
    //loading from the HelpSource directory
    ~model_path = PathName(RTNeural.filenameSymbol.asString).upMultiDir(2)++"HelpSource/audio_models/";

    RTNeural.loadModel(~synth, \this_one, ~model_path++"JoyoExtremeMetal_RTNeural.json", 1);
)

//GRU model of a Muff pedal. This training using a GRU with 64 hidden layers
(
    RTNeural.loadModel(~synth, \this_one, ~model_path++"Muff_GRU_RTNeural.json", 1);
)

//LSTM and GRU models are trained at a sample rate, and that needs to be correctly indicated in the synthdef
//notice the sonic difference when the sample rate is wrong
SynthDef('rt_neural_wrong_sr', {
    var in = PlayBuf.ar(1, \buf_num.kr(0), BufRateScale.kr(\buf_dur.kr), loop:1);
    var synth = RTNeural.ar([in], 1, 'this_one', \bypass.kr(0), \sr.kr(96000))+in*0.5;
    Out.ar(0, synth.dup);
}).load(s);

~synth = Synth("rt_neural_wrong_sr");

RTNeural.loadModel(~synth, \this_one, ~model_path++"JoyoExtremeMetal_RTNeural.json", 1);


//some AGAM models have 2 or more inputs for "Gain Knobs" and such
(
    SynthDef('rt_neural_dist_2d', {
    var in = PlayBuf.ar(1, \buf_num.kr(0), BufRateScale.kr(\buf_dur.kr), loop:1);

    var gain = K2A.ar(MouseX.kr);

    var synth = RTNeural.ar([in, gain], 1, 'this_one', \bypass.kr(0), \sr.kr(44100))+in*0.5;
    Out.ar(0, synth.dup);
}).load(s);
)

~synth = Synth('rt_neural_dist_2d');

// move the mouse left to right to adjust the effect
(
    RTNeural.loadModel(~synth, \this_one, ~model_path++"DOD_AmericanMetal_GainKnob_RTNeural.json");
)

::

The Chow Centaur Gain Stage

code::
//the gru model from the chowdsp centaur pedal is provided in the RTNeural_python folder
//this only models the gain stage of the centaur guitar pedel, not the entire pedal

SynthDef('rtneural_centaur', {
    var rtn, in = SinOsc.ar(MouseY.kr(100, 2000));
    //centaur takes 3 inputs - audio, gain, and 1/sample_rate
    in = [in, K2A.ar(MouseX.kr), K2A.ar(1/SampleRate.ir)];
    rtn = RTNeural.ar(in, 1, 'this_one', \bypass.kr(0));
    Out.ar(0,
        rtn.dup*0.2
    );
}).load(s);

~synth = Synth('rtneural_centaur');
(
//load the small model - has a little error in the low range
~python_path = "/Users/spluta1/Desktop/RTNeural_python";  //put the correct path of RTNeural_python here
RTNeural.loadModel(~synth, \this_one, ~python_path++"/chowdhury_models/chow_centaur/centaur.json");
)
//the large model clears up the issue in the low register
RTNeural.loadModel(~synth, \this_one, ~python_path++"/chowdhury_models/chow_centaur/centaur_large.json");

//check out the excellent distortion shape
Stethoscope(s, 2);
::

See the lstm_predict_tutorial on how to use RTNeural as an RNN note predictor!!

----------------Multi Layer Perceptron---------------

Using the UGen as a Control Rate Multi Layer Perceptron

See the mlp_control_tutorial for a much more comprehensive overview of this functionality and the mlp_adding_tutorial for a simple example of MLP training

code::
//a control-rate Multi Layer Perceptron that uses mouse input to give ten channels of control outputs
(
SynthDef('rtneural_mlp_control', {
    var rtn, in = [MouseX.kr, MouseY.kr];
    
    rtn = RTNeural.kr(in, 13, 'mlp', \trig_mode.kr(0), 0, \bypass.kr(0));
    rtn.poll;
    nil
}).load(s);
)
//control rate is very efficient - uses very little cpu
(
    ~synth = Synth('rtneural_mlp_control');

    ~model_path = PathName(RTNeural.filenameSymbol.asString).upMultiDir(2)++"HelpSource/mlp_control_tutorial/";
    RTNeural.loadModel(~synth, \mlp, (~model_path++"synth_RTNeural.json"));
)
::

An alternate approach is to use an audio rate RTNeural UGen in trigger mode. This will only run inference on samples where the trigger signal is above 0
code::
(
SynthDef('rtneural_mlp_trig', {
    //3 inputs - 1 and 2 are inputs to the NN, 3 is an audio rate trigger
    var rtn, in = [SinOsc.ar(0.1).range(0,1), SinOsc.ar(0.133).range(0,1)];
    
    //this can also operate at audio rate, but it will be very inefficient
    rtn = RTNeural.ar(in, 13, 'mlp', \bypass.kr(0), -1, \trig_mode.kr(1), Dust.ar(1));
    rtn.poll;
    nil
}).load(s);
)

//depending on the trig rate, can be even more efficient than control rate
(
    ~synth = Synth('rtneural_mlp_trig');

    ~model_path = PathName(RTNeural.filenameSymbol.asString).upMultiDir(2)++"HelpSource/mlp_control_tutorial/";
    RTNeural.loadModel(~synth, \mlp, (~model_path++"synth_RTNeural.json"));
)
::


A Neural Wavetable Oscillator

code::
//a multilayer perceptron neural network that has been trained on variable wavetable oscillator
//see RTNeural_python/4Osc_MLP_torch to view how this model was trained
//i am not saying this is a good use of a neural network

//this uses neural networks to create variable "wavetable" oscillators
//trained using pytorch
(
SynthDef('rtneural_osc_torch', {
    var rtn, in = LFSaw.ar(MouseX.kr(50, 500)).range(0,1);
    in = [in, K2A.ar(MouseY.kr)];
    rtn = RTNeural.ar(in, 1, 'this_one', \trig_mode.kr(0), \bypass.kr(0));
    Out.ar(0,
        rtn.dup*0.2
    );
}).load(s);
)

//start the synth
~synth = Synth('rtneural_osc_torch');

//load the model
~model_path = PathName(RTNeural.filenameSymbol.asString).upMultiDir(2)++"HelpSource/audio_models/";
RTNeural.loadModel(~synth, \this_one, ~model_path++"4Osc_torch_RTNeural.json");


Stethoscope(s, 2)
::

See the lstm_predict_tutorial on how to use RTNeural as an RNN note predictor!!
