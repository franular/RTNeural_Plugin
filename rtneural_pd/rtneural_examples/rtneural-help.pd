#N canvas 100 166 912 471 12;
#N canvas 837 231 1032 821 mlp_control_training 1;
#X msg 494 151 set_learn_rate 0.001;
#X msg 494 69 clear_points;
#X msg 494 126 set_epochs 5000;
#X msg 494 264 bypass 1;
#X obj 494 359 array get \$0-array2;
#X listbox 494 384 20 0 0 0 - - - 0;
#X obj 494 324 bng 30 250 50 0 empty empty empty 0 -10 0 12 #dfdfdf #000000 #000000;
#X listbox 739 384 20 0 0 0 - - - 0;
#X obj 739 433 list trim;
#X obj 739 359 array get \$0-array10;
#X obj 739 408 list prepend add_output;
#X obj 494 408 list prepend add_input;
#X obj 494 433 list trim;
#X text 780 321 add an output set from array10, f 17;
#X msg 494 473 remove_point 3;
#X text 473 510 5) write the configuration to a json file;
#X msg 165 291 post_points;
#X text 164 253 print stored point data to Pd console, f 18;
#X obj 66 352 rtneural 2 10;
#X obj 616 69 s \$0-to_rtneural;
#X obj 472 201 s \$0-to_rtneural;
#X obj 588 433 s \$0-to_rtneural;
#X obj 494 559 s \$0-to_rtneural;
#X obj 66 377 array set \$0-array10;
#N canvas 0 50 450 250 (subpatch) 0;
#X array \$0-array10 10 float 2;
#X coords 0 1 10 0 400 200 1 0 0;
#X restore 40 433 graph;
#N canvas 0 50 450 250 (subpatch) 0;
#X array \$0-array2 2 float 2;
#X coords 0 1 2 0 40 200 1 0 0;
#X restore 40 96 graph;
#X obj 591 264 s \$0-to_rtneural;
#X text 473 300 4) configure input and output data points;
#X text 473 240 3) bypass the model;
#X text 473 102 2) set the the training data variables: epochs \, learn_rate \, layers_data, f 72;
#X text 473 45 1) clear the internal data points;
#X text 552 321 add an input set from array2, f 16;
#X text 416 20 [rtneural] can also be used to make a new training:;
#X obj 66 327 r \$0-to_rtneural;
#X text 611 473 remove the set at index 3 of both input and output;
#X text 473 599 6) finally \, train the configured model via the command line as described in RTNeural_python/MLP_control/README.md (NOTE: this may take a while...), f 75;
#X msg 494 176 set_layers_data 20 relu 20 relu 10 sigmoid;
#X text 485 648 from the RTNeural_python directory in the terminal: 1) make sure you are in a virtual environment as described by RTNeural_python/README.md 2) run 'python MLP_control/mlp_control_train_convert.py -f <your file>' to train the network the file will be saved as <your file without the extension>_RTNeural.json in the same directory as <your file>, f 61;
#X msg 494 534 write_json saved_models/mlp/mlp_training.json;
#X connect 0 0 20 0;
#X connect 1 0 19 0;
#X connect 2 0 20 0;
#X connect 3 0 26 0;
#X connect 4 0 5 0;
#X connect 5 0 11 0;
#X connect 6 0 4 0;
#X connect 6 0 9 0;
#X connect 7 0 10 0;
#X connect 8 0 21 0;
#X connect 9 0 7 0;
#X connect 10 0 8 0;
#X connect 11 0 12 0;
#X connect 12 0 21 0;
#X connect 14 0 21 0;
#X connect 16 0 18 0;
#X connect 18 0 23 0;
#X connect 33 0 18 0;
#X connect 36 0 20 0;
#X connect 38 0 22 0;
#X restore 21 149 pd mlp_control_training;
#N canvas 675 118 893 413 mlp_control 0;
#X obj 397 20 vsl 19 162 0 1 0 0 empty empty empty 0 -9 0 12 #dfdfdf #000000 #000000 0 1;
#X obj 422 20 vsl 19 162 0 1 0 0 empty empty empty 0 -9 0 12 #dfdfdf #000000 #000000 0 1;
#X obj 412 215 pack;
#X obj 412 190 t b f;
#X obj 227 293 rtneural 2 10;
#X obj 429 296 tgl 30 0 empty empty empty 0 -10 0 12 #dfdfdf #000000 #000000 0 1;
#X msg 429 355 bypass \$1;
#N canvas 0 0 450 300 (subpatch) 0;
#X array \$0-neural_out 10 float 2;
#X coords 0 1 10 0 300 150 1 0 0;
#X restore 27 46 graph;
#X text 471 191 Load the model file at the given (relative or absolute) path. The model must satisfy [rtneural]'s channel counts.;
#X text 21 208 [rtneural] runs inference from a pre-trained neural net on an input list. It accepts two creation arguments: input list length and output list length., f 44;
#X text 471 290 set whether [rtneural]'s processing is bypassed (if set to 1 \, no output values are generated when input values are received), f 52;
#X floatatom 429 331 5 0 0 0 - - - 0;
#X obj 227 318 array set \$0-neural_out;
#X text 470 56 [rtneural] expects an input list of the declared length. The values are controlled by these faders., f 50;
#X msg 472 231 load_model saved_models/mlp/mlp_training_RTNeural.json;
#X connect 0 0 2 0;
#X connect 1 0 3 0;
#X connect 2 0 4 0;
#X connect 3 0 2 0;
#X connect 3 1 2 1;
#X connect 4 0 12 0;
#X connect 5 0 11 0;
#X connect 6 0 4 0;
#X connect 11 0 6 0;
#X connect 14 0 4 0;
#X restore 21 116 pd mlp_control;
#X text 30 15 rtneural is a data rate neural inferencing external which uses the RTNeural inference engine to load and run tensorflow and pytorch trained neural network models of any shape or size.;
#X text 195 115 <-- multi layer perceptron inference;
#X text 195 149 <-- learn how to make a new mlp training;
#N canvas -2 374 1496 578 rnn_note_prediction 0;
#X obj 600 109 loadbang;
#N canvas 469 494 450 278 (subpatch) 0;
#X array \$0-softmax 84 float 3;
#A 0 1.75525e-08 1.53221e-08 1.71885e-08 2.25268e-08 2.01079e-08 2.08262e-08 2.08279e-08 1.7348e-08 2.19951e-08 1.66915e-08 1.83219e-08 2.20607e-08 2.10096e-08 2.32806e-08 1.49577e-08 1.68934e-08 2.06199e-08 2.10597e-08 1.88758e-08 2.04149e-08 1.82925e-08 1.57656e-08 2.14028e-08 1.93559e-08 2.20472e-08 1.90365e-08 2.29285e-08 2.13601e-08 2.17216e-08 2.04496e-08 2.04647e-08 2.22612e-08 1.93604e-08 1.97753e-08 2.07336e-08 1.79659e-08 2.03034e-08 2.07366e-08 1.93571e-08 2.45372e-08 1.99407e-08 1.88503e-08 2.29041e-08 1.73484e-08 2.15176e-08 2.3367e-08 2.20886e-08 1.78382e-08 2.10683e-08 2.2076e-08 1.90731e-08 2.11469e-08 2.12979e-08 2.27061e-08 2.38727e-08 2.36141e-08 2.07671e-08 2.41558e-08 1.87925e-08 1.84625e-08 1.93109e-08 2.04486e-08 1.31641e-05 1.92658e-08 1.89357e-06 2.02118e-08 0.537341 0.0186094 1.9041e-08 0.0193256 1.99539e-08 0.298598 0.112706 0.0121482 0.000196178 2.56123e-08 0.000539969 1.09783e-07 0.000392883 0.00012249 2.27646e-08 2.72565e-06 1.8337e-08 4.78979e-08;
#X coords 0 1 84 0 700 200 1 0 0;
#X restore 741 306 graph;
#X obj 376 353 rtneural 1 84;
#X msg 544 241 reset;
#N canvas 323 154 1173 672 get_5 0;
#X obj 70 6 inlet;
#X msg 70 76 79 79 81 79 81 83 81 79 78 76 74 67 66 67 69 67 66 67 69 67 69 67 69 67 69 67 69 67 66 67 69 67 66 67 66 64 64 62 74 74 76 74 76 77 76 74 72 71 69 79 78 76 78 79 78 81 79 78 76 74 72 72 81 72 71 67 66 66 67 66 67 71 71 73 71 73 74 74 73 71 69 67 71 76 79 81 79 78 79 81 79 81 79 81 79 81 79 81 79 78 79 79 78 76 74 73 71 73 74 73 74 73 74 73 76 81 79 78 76 74 69 71 72 71 69 67 66 64 74 73 74 76 74 73 71 69 79 71 73 73 74 76 74 73 73 74 69 67 69 71 69 67 69 71 69 71 69 71 69 71 69 71 69 71 72 74 76 76 74 72 71 69 67 79 78 76 78 75 75 76 75 76 81 83 81 79 81 78 76 75 71 76 78 79 78 78 76 71 72 71 69 71 79 76 78 75 75 76 67 66 64 66 76 76 75 78 79 81 79 78 78 76 78 75 75 76 76 74 72 71 69 71 72 74 72 71 72 71 69 69 67 69 71 72 74 72 71 72 69 64 69 72 74 72 71 72 69 66 69 72 76 74 72 71 72 71 69 71 67 62 67 71 67 72 74 76 77 76 74 76 72 67 72 76 72 78 79 81 72 71 69 71 72 74 67 71 69 67 66 67 62 67 66 66 67 69 67 69 71 69 67 69 71 69 71 69 71 69 71 69 71 69 71 72 74 76 76 74 72 71 69 67 79 78 76 78 75 75 76 75 76 81 83 81 79 81 78 76 75 71 76 78 79 78 78 76 71 72 71 69 71 79 76 78 75 75 76 67 66 64 66 76 76 75 78 79 81 79 78 78 76 78 75 75 76 76 74 72 71 69 71 72 74 72 71 72 71 69 69 67 69 71 72 74 72 71 72 69 64 69 72 74 72 71 72 69 66 69 72 76 74 72 71 72 71 69 71 67 62 67 71 67 72 74 76 77 76 74 76 72 67 72 76 72 78 79 81 72 71 69 71 72 74 67 71 69 67 66 67 62 67 66 66 67;
#X obj 54 831 list split;
#X obj 80 502 list split;
#X obj 158 426 random 70;
#X obj 155 464 + 5;
#X listbox 200 575 29 0 0 0 - - - 0;
#X obj 179 16 bng 19 250 50 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000;
#X obj 64 841 list split;
#X obj 98 540 list split 5;
#X obj 568 156 until;
#X obj 577 270 list store;
#X obj 568 181 f;
#X obj 606 182 + 1;
#X msg 625 156 0, f 2;
#X msg 564 243 get \$1 1;
#X obj 595 294 / 83;
#X obj 586 410 list store;
#X msg 595 384 set \$2 \$1;
#X obj 579 322 pack;
#X obj 566 212 t f f;
#X obj 589 349 t b l;
#X obj 602 123 t b b l b, f 25;
#X obj 103 619 outlet;
#X obj 628 523 outlet;
#X listbox 717 516 47 0 0 0 - - - 0;
#X obj 70 42 t b b b;
#X connect 0 0 26 0;
#X connect 1 0 3 0;
#X connect 3 1 9 0;
#X connect 4 0 5 0;
#X connect 5 0 3 1;
#X connect 7 0 26 0;
#X connect 9 0 6 0;
#X connect 9 0 23 0;
#X connect 9 0 22 0;
#X connect 10 0 12 0;
#X connect 11 0 16 0;
#X connect 11 1 10 1;
#X connect 12 0 13 0;
#X connect 12 0 20 0;
#X connect 13 0 12 1;
#X connect 14 0 12 1;
#X connect 15 0 11 0;
#X connect 16 0 19 0;
#X connect 17 0 24 0;
#X connect 17 0 25 0;
#X connect 18 0 17 0;
#X connect 19 0 21 0;
#X connect 20 0 15 0;
#X connect 20 1 19 1;
#X connect 21 1 18 0;
#X connect 22 0 10 0;
#X connect 22 1 14 0;
#X connect 22 2 11 1;
#X connect 22 2 17 1;
#X connect 22 3 17 1;
#X connect 26 0 17 0;
#X connect 26 1 1 0;
#X connect 26 2 4 0;
#X restore 276 142 pd get_5;
#X obj 350 68 bng 19 250 50 0 empty empty empty 0 -10 0 12 #fcfcfc #000000 #000000;
#X listbox 230 276 43 0 0 0 - - - 0;
#X listbox 231 205 43 0 0 0 - - - 0;
#X obj 376 392 array set \$0-softmax;
#X obj 352 103 t b b;
#X text 1116 48 the last 20 outputs of rtneural and rtneural~ softmax;
#X text 78 10 this shows how the rtneural plugin outputs a softmax array to help predict the next note in a time series, f 61;
#X text 596 241 a reset has to be sent before each 5 note input;
#X text 100 258 the 5 note sequence normalized between 0 and 1;
#X text 63 345 the highest note in the series is 83 \, so the network must have a size of 84;
#X text 993 281 the probability of each note (up to midi 83);
#X msg 600 140 load_model saved_models/lstm_note_prediction/bach_RTNeural.json, f 63;
#X connect 0 0 16 0;
#X connect 2 0 8 0;
#X connect 3 0 2 0;
#X connect 4 0 7 0;
#X connect 4 1 6 0;
#X connect 5 0 9 0;
#X connect 6 0 2 0;
#X connect 9 0 4 0;
#X connect 9 1 3 0;
#X connect 16 0 2 0;
#X restore 21 182 pd rnn_note_prediction;
#X text 195 183 <-- note prediction using an lstm model;
